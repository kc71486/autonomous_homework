{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUEHFcHoQFu9"
      },
      "source": [
        "# global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "CD_SWJqCLz1s"
      },
      "outputs": [],
      "source": [
        "# sneaky global variable !!!!!\n",
        "\n",
        "# bump this version every time new install is added/removed/modified\n",
        "gl_latest_env_version = \"1.0.3\";\n",
        "\n",
        "# dataset root\n",
        "gl_dataset_directory = \"dataset/\";\n",
        "\n",
        "# weight root\n",
        "gl_weight_directory = \"weights/\";\n",
        "\n",
        "# training/validation/testing file location\n",
        "gl_training_file = gl_dataset_directory + \"train.p\";\n",
        "gl_validation_file = gl_dataset_directory + \"valid.p\";\n",
        "gl_testing_file = gl_dataset_directory + \"test.p\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmyWe6y2Bz1Y"
      },
      "source": [
        "# environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "6Y6F5iUNBp-X"
      },
      "outputs": [],
      "source": [
        "def install_env_wrapper() -> None:\n",
        "    \"\"\"\n",
        "    Update environment and throw error if not already installed.\n",
        "\n",
        "    This is a standalone function, doesn't require external import.\n",
        "    \"\"\"\n",
        "    env_result = install_env(gl_latest_env_version);\n",
        "    if (env_result != True):\n",
        "        print(\"Changes might not apply immediately, restart runtime to apply changes.\");\n",
        "        assert(False); # force error\n",
        "\n",
        "def install_env(latest_version: str) -> bool | None:\n",
        "    \"\"\"\n",
        "    Return true if already installed, false if not, None (or error) if error.\n",
        "    \"\"\"\n",
        "    import os;\n",
        "    import shutil;\n",
        "\n",
        "    env_file = \"env.txt\";\n",
        "    env_version = \"0.0.0\"; # default version\n",
        "    if os.path.isfile(env_file):\n",
        "        with open(env_file, \"r\") as f:\n",
        "            version = f.readline()[:-1];\n",
        "            env_version = version;\n",
        "    else:\n",
        "        with open(env_file, \"x\") as f:\n",
        "            f.write(env_version + \"\\n\");\n",
        "\n",
        "    if env_version == gl_latest_env_version:\n",
        "        print(\"version matched, install skipped.\");\n",
        "        return True;\n",
        "\n",
        "    print(\"installing package...\");\n",
        "    install_package(\"tensorflow\");\n",
        "    install_package(\"keras\");\n",
        "    install_package(\"pandas\");\n",
        "    install_package(\"matplotlib\");\n",
        "\n",
        "    if in_colab():\n",
        "        if os.path.isdir(\"drive\"):\n",
        "            print(\"drive already mounted.\");\n",
        "        else:\n",
        "            print(\"mounting drive...\");\n",
        "            from google.colab import drive;\n",
        "            drive.mount(\"./drive\", force_remount=False);\n",
        "\n",
        "    if os.path.isdir(gl_dataset_directory):\n",
        "        print(gl_dataset_directory + \" is up to date.\");\n",
        "    else:\n",
        "        if in_colab():\n",
        "            print(\"setup dataset...\");\n",
        "            srcpath = \"./drive/MyDrive/autonomous_project2/\" + gl_dataset_directory;\n",
        "            shutil.copytree(src=srcpath,\n",
        "                            dst=gl_dataset_directory);\n",
        "        else:\n",
        "            os.mkdir(gl_dataset_directory);\n",
        "            print(gl_dataset_directory + \" not found, please manually place it.\");\n",
        "            return None;\n",
        "\n",
        "    if os.path.isdir(gl_weight_directory):\n",
        "        print(gl_weight_directory + \" is up to date.\");\n",
        "    else:\n",
        "        if in_colab():\n",
        "            srcpath = \"./drive/MyDrive/autonomous_project2/\" + gl_dataset_directory;\n",
        "            if not os.path.isdir(srcpath):\n",
        "                print(\"creating \" + srcpath + \"...\");\n",
        "                os.mkdir(srcpath);\n",
        "            else:\n",
        "                print(\"creating \" + gl_weight_directory + \"...\");\n",
        "                os.mkdir(gl_weight_directory);\n",
        "            print(\"setup weight...\");\n",
        "            shutil.copytree(src=srcpath,\n",
        "                            dst=gl_dataset_directory);\n",
        "        else:\n",
        "            print(\"creating \" + gl_weight_directory + \"...\");\n",
        "            os.mkdir(gl_weight_directory);\n",
        "\n",
        "    with open(env_file, \"w\") as f:\n",
        "        f.write(gl_latest_env_version + \"\\n\");\n",
        "    print(\"version updated\");\n",
        "    return False;\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    import importlib.util;\n",
        "    return importlib.util.find_spec(\"google.colab\") is not None;\n",
        "\n",
        "def install_package(package_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Automatically install package.\n",
        "    \"\"\"\n",
        "    import importlib.util;\n",
        "    import subprocess;\n",
        "    if importlib.util.find_spec(package_name):\n",
        "        command = [\"pip\", \"install\", package_name];\n",
        "        print(\"installing \"+ package_name + \"...\");\n",
        "        proc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT);\n",
        "        _ = proc.communicate(input='y'.encode())[0];"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhYRehM7CkTi"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TGy1Z83dCjgo"
      },
      "outputs": [],
      "source": [
        "import numpy as np;\n",
        "\n",
        "import pickle;\n",
        "\n",
        "import keras;\n",
        "from keras.src.models.model import Model;\n",
        "from keras.src.optimizers.optimizer import Optimizer;\n",
        "\n",
        "import matplotlib.pyplot as plt;\n",
        "\n",
        "from typing import Callable, Any;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QPwcpWgCKHP"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2ifhicnU2ZA"
      },
      "source": [
        "## load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_CGDwz340Gaw"
      },
      "outputs": [],
      "source": [
        "def loadDataset(usage):\n",
        "    if usage == \"train\":\n",
        "        use_file = gl_training_file;\n",
        "    elif usage == \"valid\":\n",
        "        use_file = gl_validation_file;\n",
        "    elif usage == \"test\":\n",
        "        use_file = gl_testing_file;\n",
        "    else:\n",
        "        raise ValueError;\n",
        "    with open(use_file, mode=\"rb\") as f:\n",
        "        dataset = pickle.load(f);\n",
        "    return dataset[\"features\"], dataset[\"labels\"];"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59u1soOFEPix"
      },
      "source": [
        "## preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "-llFd6W8Jur_"
      },
      "outputs": [],
      "source": [
        "def preprocess(X: np.ndarray, Y: np.ndarray, num_classes: int | None) -> tuple[np.ndarray, np.ndarray]:\n",
        "    if num_classes is None:\n",
        "        num_classes = len(np.unique(Y));\n",
        "    Y_OneHot = keras.utils.to_categorical(Y, num_classes=num_classes);\n",
        "    Y_Reshape = Y_OneHot.reshape((-1, 1, 1, num_classes));\n",
        "\n",
        "    return X, Y_Reshape;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2nzO6pKYI-7"
      },
      "source": [
        "## define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "wbbVkcA2YLf_"
      },
      "outputs": [],
      "source": [
        "def Lenet(n_classes: int) -> Model:\n",
        "    model = keras.Sequential();\n",
        "    model.add(keras.layers.BatchNormalization());\n",
        "    model.add(keras.layers.Conv2D(6, (5, 5), activation=keras.activations.tanh));\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)));\n",
        "    model.add(keras.layers.Conv2D(16, (5, 5), activation=keras.activations.tanh));\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)));\n",
        "    model.add(keras.layers.Conv2D(120, (5, 5), activation=keras.activations.tanh));\n",
        "\n",
        "    model.add(keras.layers.Dense(84, activation=keras.activations.tanh));\n",
        "    model.add(keras.layers.Dense(n_classes, activation=keras.activations.softmax));\n",
        "    return model;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "71FagJRXMPDg"
      },
      "outputs": [],
      "source": [
        "def getModel(model_fn: Callable[[int], Model], n_classes: int, optimizer: Optimizer) -> Model:\n",
        "    model = model_fn(n_classes=n_classes);\n",
        "    model.compile(optimizer=optimizer,\n",
        "                loss=keras.losses.CategoricalCrossentropy(),\n",
        "                run_eagerly=False);\n",
        "    return model;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Skr1YbECJH"
      },
      "source": [
        "## train, evaluate and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainModel(model: Model, x_train, y_train, x_valid, y_valid, batch_size: int, epochs: int) -> Any:\n",
        "    return model.fit(x=x_train,\n",
        "                     y=y_train,\n",
        "                     batch_size=batch_size,\n",
        "                     epochs=epochs,\n",
        "                     validation_data=(x_valid, y_valid),\n",
        "                     );\n",
        "\n",
        "def testModel(model: Model, x_test, y_test, batch_size: int) -> Any:\n",
        "    return model.evaluate(x=x_test,\n",
        "                          y=y_test,\n",
        "                          batch_size=batch_size,\n",
        "                          );\n",
        "\n",
        "def predictModel(model: Model, x_test) -> np.ndarray:\n",
        "    return model.predict(x=x_test,\n",
        "                         batch_size=1,\n",
        "                         );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## save and load weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def saveWeight(model: Model, filename: str) -> None:\n",
        "    filepath = gl_weight_directory + filename + \".weights.h5\";\n",
        "    model.save_weights(filepath=filepath, overwrite=True);\n",
        "\n",
        "def loadWeight(model: Model, filename: str) -> None:\n",
        "    filepath = gl_weight_directory + filename + \".weights.h5\";\n",
        "    model.load_weights(filepath=filepath, skip_mismatch=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test inference image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSignNames() -> np.ndarray:\n",
        "    import pandas;\n",
        "    df = pandas.read_csv('./signnames.csv')['SignName'];\n",
        "    return df.to_numpy();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the images and plot them here.\n",
        "def inference(sign_names: np.ndarray, model: keras.models.Sequential) -> None:\n",
        "    import matplotlib.image as mpimg;\n",
        "    NImages = 10\n",
        "    X_real = np.zeros((NImages,32,32,3)).astype(np.uint8);\n",
        "    y_real = np.array([17,12,14,11,38,4,35,33,25,13]);\n",
        "    prediction = predictModel(model, X_real);\n",
        "    prediction = prediction.reshape((NImages, -1));\n",
        "    prediction_class = np.argmax(prediction, axis=1);\n",
        "    prediction_value = np.max(prediction, axis=1);\n",
        "\n",
        "    for i in range(NImages):\n",
        "        image = mpimg.imread('./testImages/'+str(i+1)+'.png');\n",
        "        image = np.multiply(image,256).astype(np.uint8)[:,:,0:3];\n",
        "        X_real[i] = image;\n",
        "        printfmt = \"predicted class = {} ({}), value={:.4f}\\nactual class = {} ({}).\"\n",
        "        printstring = printfmt.format(prediction_class[i],\n",
        "                                      sign_names[prediction_class[i]],\n",
        "                                      prediction_value[i],\n",
        "                                      y_real[i],\n",
        "                                      sign_names[y_real[i]],\n",
        "                                      );\n",
        "        print(printstring);\n",
        "    for i in range(NImages):\n",
        "        plt.figure(figsize=(1,1));\n",
        "        plt.imshow(X_real[i]);\n",
        "        plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "gl_history = None;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main() -> None:\n",
        "    \"\"\"\n",
        "    main function\n",
        "    \"\"\"\n",
        "    # start\n",
        "    print(\"main begin\");\n",
        "\n",
        "    # hyperparameters\n",
        "    batch_size = 16;\n",
        "    epochs = 10;\n",
        "\n",
        "    # do final test or not\n",
        "    do_final_test = True;\n",
        "\n",
        "    # load dataset\n",
        "    X_train_raw, Y_train_raw = loadDataset(\"train\");\n",
        "    X_valid_raw, Y_valid_raw = loadDataset(\"valid\");\n",
        "    X_test_raw, Y_test_raw = loadDataset(\"test\");\n",
        "\n",
        "    # preprocess dataset\n",
        "    num_classes = len(np.unique(Y_train_raw)); # 43\n",
        "    X_train, Y_train = preprocess(X_train_raw, Y_train_raw, num_classes);\n",
        "    X_valid, Y_valid = preprocess(X_valid_raw, Y_valid_raw, num_classes);\n",
        "    X_test, Y_test = preprocess(X_test_raw, Y_test_raw, num_classes);\n",
        "\n",
        "    # get model\n",
        "    sgd = keras.optimizers.SGD(learning_rate=0.01);\n",
        "    adam = keras.optimizers.Adam(learning_rate=0.001);\n",
        "    model = getModel(model_fn=Lenet, n_classes=num_classes, optimizer=sgd);\n",
        "\n",
        "    # train model\n",
        "    history = trainModel(model, X_train, Y_train, X_valid, Y_valid, batch_size, epochs);\n",
        "    global gl_history;\n",
        "    gl_history = history;\n",
        "\n",
        "    # save weight(optional)\n",
        "    saveWeight(model, \"lenet_v1\");\n",
        "\n",
        "    # evaluate model\n",
        "    print(\"=====evaluate model result:======\");\n",
        "    testModel(model, X_valid, Y_valid, batch_size);\n",
        "    print(\"=================================\");\n",
        "\n",
        "    # test model\n",
        "    if do_final_test:\n",
        "        print(\"=======test model result:=======\");\n",
        "        testModel(model, X_test, Y_test, batch_size);\n",
        "        print(\"================================\");\n",
        "\n",
        "    # inference\n",
        "    if do_final_test:\n",
        "        sign_names = getSignNames();\n",
        "        inference(sign_names=sign_names, model=model);\n",
        "\n",
        "    # finish\n",
        "    print(\"main end\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "-B6Vg70LLx6I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "version matched, install skipped.\n",
            "main begin\n",
            "Epoch 1/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.3271 - val_loss: 0.7974\n",
            "Epoch 2/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.5355 - val_loss: 0.4532\n",
            "Epoch 3/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.2755 - val_loss: 0.3307\n",
            "Epoch 4/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.1804 - val_loss: 0.2712\n",
            "Epoch 5/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.1295 - val_loss: 0.2446\n",
            "Epoch 6/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0976 - val_loss: 0.2239\n",
            "Epoch 7/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0814 - val_loss: 0.2350\n",
            "Epoch 8/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.0655 - val_loss: 0.1936\n",
            "Epoch 9/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0541 - val_loss: 0.1912\n",
            "Epoch 10/10\n",
            "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0463 - val_loss: 0.1859\n",
            "=====evaluate model result:======\n",
            "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2139\n",
            "=================================\n",
            "=======test model result:=======\n",
            "\u001b[1m790/790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2241\n",
            "================================\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 17 (No entry).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 12 (Priority road).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 14 (Stop).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 11 (Right-of-way at the next intersection).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 38 (Keep right).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 4 (Speed limit (70km/h)).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 35 (Ahead only).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 33 (Turn right ahead).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 25 (Road work).\n",
            "predicted class = 3 (Speed limit (60km/h)), value=0.3861\n",
            "actual class = 13 (Yield).\n",
            "main end\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    install_env_wrapper();\n",
        "    main();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
