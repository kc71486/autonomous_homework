{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUEHFcHoQFu9"
      },
      "source": [
        "# global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CD_SWJqCLz1s"
      },
      "outputs": [],
      "source": [
        "# sneaky global variable !!!!!\n",
        "\n",
        "# bump this version every time new install is added/removed/modified\n",
        "gl_latest_env_version = \"1.1.1\";\n",
        "\n",
        "# if using drive\n",
        "gl_drive_root = \"./drive/MyDrive/autonomous_project2/\";\n",
        "\n",
        "# required files\n",
        "gl_dataset_directory = \"dataset/\";\n",
        "gl_weight_directory = \"weights/\";\n",
        "gl_testimages_directory = \"testImages/\";\n",
        "gl_signnames_file = \"signnames.csv\";\n",
        "\n",
        "# number of test images\n",
        "gl_num_images = 10;\n",
        "\n",
        "# training/validation/testing file location\n",
        "gl_training_file = gl_dataset_directory + \"train.p\";\n",
        "gl_validation_file = gl_dataset_directory + \"valid.p\";\n",
        "gl_testing_file = gl_dataset_directory + \"test.p\";\n",
        "\n",
        "# convert to .py for windows user without anaconda\n",
        "# jupyter-nbconvert --to python project.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmyWe6y2Bz1Y"
      },
      "source": [
        "# environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Y6F5iUNBp-X"
      },
      "outputs": [],
      "source": [
        "def installEnvWrapper() -> None:\n",
        "    \"\"\"\n",
        "    Update environment and throw error if not already installed.\n",
        "\n",
        "    This is a standalone function, doesn't require external import.\n",
        "    \"\"\"\n",
        "    env_result = installEnv(gl_latest_env_version);\n",
        "    if (env_result != True):\n",
        "        print(\"Changes might not apply immediately, restart runtime to apply changes.\");\n",
        "        assert(False); # force error\n",
        "\n",
        "def installEnv(latest_version: str) -> bool | None:\n",
        "    \"\"\"\n",
        "    Return true if already installed, false if not, None (or error) if error.\n",
        "    \"\"\"\n",
        "    import os;\n",
        "    import shutil;\n",
        "\n",
        "    env_file = \"env.txt\";\n",
        "    env_version = \"0.0.0\"; # default version\n",
        "    if os.path.isfile(env_file):\n",
        "        with open(env_file, \"r\") as f:\n",
        "            version = f.readline()[:-1];\n",
        "            env_version = version;\n",
        "    else:\n",
        "        with open(env_file, \"x\") as f:\n",
        "            f.write(env_version + \"\\n\");\n",
        "\n",
        "    if env_version == gl_latest_env_version:\n",
        "        print(\"version matched, install skipped.\");\n",
        "        return True;\n",
        "\n",
        "    print(\"installing package...\");\n",
        "    installPackage(\"notebook\");\n",
        "    installPackage(\"jupyter\");\n",
        "    installPackage(\"opencv-python\");\n",
        "    installPackage(\"tensorflow\");\n",
        "    installPackage(\"keras\");\n",
        "    installPackage(\"pandas\");\n",
        "    installPackage(\"matplotlib\");\n",
        "\n",
        "    if inColab():\n",
        "        if os.path.isdir(gl_drive_root):\n",
        "            print(\"drive already mounted.\");\n",
        "        else:\n",
        "            print(\"mounting drive...\");\n",
        "            from google.colab import drive;\n",
        "            drive.mount(\"./drive\", force_remount=False);\n",
        "\n",
        "    fileok = True;\n",
        "    fileok = fileok and checkFile(gl_dataset_directory, autocreate=False);\n",
        "    fileok = fileok and checkFile(gl_weight_directory, autocreate=True);\n",
        "    fileok = fileok and checkFile(gl_testimages_directory, autocreate=False);\n",
        "    fileok = fileok and checkFile(gl_signnames_file, autocreate=False);\n",
        "    if not fileok:\n",
        "        return False;\n",
        "\n",
        "    with open(env_file, \"w\") as f:\n",
        "        f.write(gl_latest_env_version + \"\\n\");\n",
        "    print(\"version updated\");\n",
        "    return False;\n",
        "\n",
        "def inColab() -> bool:\n",
        "    import importlib.util;\n",
        "    return importlib.util.find_spec(\"google.colab\") is not None;\n",
        "\n",
        "def installPackage(package_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Automatically install package.\n",
        "    \"\"\"\n",
        "    import importlib.util;\n",
        "    import subprocess;\n",
        "    if importlib.util.find_spec(package_name):\n",
        "        print(package_name + \" is installed.\");\n",
        "    else:\n",
        "        command = [\"pip\", \"install\", package_name];\n",
        "        print(\"installing \"+ package_name + \"...\");\n",
        "        proc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT);\n",
        "        _ = proc.communicate(input='y'.encode())[0];\n",
        "\n",
        "def checkFile(dstpath: str, autocreate: bool = False) -> bool:\n",
        "    \"\"\"\n",
        "    automatically verify files and copy if needed.\n",
        "    \"\"\"\n",
        "    import os;\n",
        "    import shutil;\n",
        "    if os.path.exists(dstpath):\n",
        "        print(dstpath + \" is up to date.\");\n",
        "    else:\n",
        "        if inColab():\n",
        "            srcpath = gl_drive_root + dstpath;\n",
        "            if autocreate and not os.path.isdir(srcpath):\n",
        "                os.makedirs(srcpath);\n",
        "                print(\"creating \" + dstpath + \"...\");\n",
        "            print(\"setup \"+ dstpath + \"...\");\n",
        "            if os.path.isdir(srcpath):\n",
        "                shutil.copytree(src=srcpath,\n",
        "                                dst=dstpath);\n",
        "            else:\n",
        "                shutil.copy(src=srcpath,\n",
        "                            dst=dstpath);\n",
        "        else:\n",
        "            if autocreate:\n",
        "                print(\"creating \" + dstpath + \"...\");\n",
        "                os.mkdir(dstpath);\n",
        "            else:\n",
        "                print(dstpath + \" not found, please manually place it.\");\n",
        "                return False;\n",
        "    return True;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbwSFsntBuSq",
        "outputId": "38a56f5e-26e7-42a7-8b32-db3ec03d7bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version matched, install skipped.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    installEnvWrapper();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhYRehM7CkTi"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TGy1Z83dCjgo"
      },
      "outputs": [],
      "source": [
        "import numpy as np;\n",
        "\n",
        "import pickle;\n",
        "import cv2;\n",
        "\n",
        "import tensorflow as tf;\n",
        "import keras;\n",
        "from keras import Model, layers, activations, metrics, losses;\n",
        "from keras.src.optimizers.optimizer import Optimizer;\n",
        "\n",
        "from typing import Callable, Any;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QPwcpWgCKHP"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2ifhicnU2ZA"
      },
      "source": [
        "## dataset & preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_CGDwz340Gaw"
      },
      "outputs": [],
      "source": [
        "def loadDataset(usage):\n",
        "    if usage == \"train\":\n",
        "        use_file = gl_training_file;\n",
        "    elif usage == \"valid\":\n",
        "        use_file = gl_validation_file;\n",
        "    elif usage == \"test\":\n",
        "        use_file = gl_testing_file;\n",
        "    else:\n",
        "        raise ValueError;\n",
        "    # image = tf.image.rgb_to_grayscale(image);\n",
        "    with open(use_file, mode=\"rb\") as f:\n",
        "        dataset = pickle.load(f);\n",
        "    return dataset[\"features\"], dataset[\"labels\"];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lq5TS4BTXxv1"
      },
      "outputs": [],
      "source": [
        "def preprocess(image, label, input_size: tuple[int, int, int]):\n",
        "    input_x, input_y, input_z = input_size;\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, (input_x, input_y));\n",
        "    if input_z == 1:\n",
        "        image = tf.image.rgb_to_grayscale(image);\n",
        "    image = (image / 255.0);\n",
        "    return image, label;\n",
        "\n",
        "def augment(image, label, input_size: tuple[int, int, int]):\n",
        "    image, label = preprocess(image, label, input_size);\n",
        "    pad_amt = int(input_size[0] / 10);\n",
        "    image = tf.image.resize_with_pad(image, input_size[0] + pad_amt, input_size[1] + pad_amt);\n",
        "    image = tf.image.random_crop(image, size=input_size);\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1);\n",
        "    image = tf.clip_by_value(image, 0, 1);\n",
        "    return image, label;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OGipxZPUXxv2"
      },
      "outputs": [],
      "source": [
        "def datasetTransform(dataset: tf.data.Dataset, map_fn, batch_size: int, shuffle: bool):\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(50000, reshuffle_each_iteration=True);\n",
        "\n",
        "    dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE);\n",
        "    dataset = dataset.batch(batch_size);\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2nzO6pKYI-7"
      },
      "source": [
        "## define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wbbVkcA2YLf_"
      },
      "outputs": [],
      "source": [
        "def Lenet(n_classes: int) -> Model:\n",
        "    model = keras.Sequential();\n",
        "    model.add(keras.Input((32, 32, 1)));\n",
        "\n",
        "    model.add(layers.Conv2D(6, (5, 5), activation=activations.sigmoid));\n",
        "    model.add(layers.AveragePooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(16, (5, 5), activation=activations.sigmoid));\n",
        "    model.add(layers.AveragePooling2D((2, 2)));\n",
        "\n",
        "    model.add(layers.Flatten());\n",
        "\n",
        "    model.add(layers.Dense(120, activation=activations.sigmoid));\n",
        "    model.add(layers.Dense(84, activation=activations.sigmoid));\n",
        "    model.add(layers.Dense(n_classes, activation=activations.softmax));\n",
        "    return model;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FFbPm21G56Mu"
      },
      "outputs": [],
      "source": [
        "def VGGFake(n_classes: int) -> Model:\n",
        "    model = keras.Sequential();\n",
        "    model.add(keras.Input((64, 64, 3)));\n",
        "\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "\n",
        "    model.add(layers.Flatten());\n",
        "\n",
        "    model.add(layers.Dense(1024, activation=activations.relu));\n",
        "    model.add(layers.Dense(1024, activation=activations.relu));\n",
        "    model.add(layers.Dense(n_classes, activation=activations.softmax));\n",
        "\n",
        "    return model;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def VGGFakePlus(n_classes: int) -> Model:\n",
        "    model = keras.Sequential();\n",
        "    model.add(keras.Input((64, 64, 3)));\n",
        "\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation=activations.relu));\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "\n",
        "    model.add(layers.Flatten());\n",
        "\n",
        "    model.add(layers.Dense(1024, activation=activations.relu));\n",
        "    model.add(layers.Dropout(0.2));\n",
        "    model.add(layers.Dense(1024, activation=activations.relu));\n",
        "    model.add(layers.Dropout(0.2));\n",
        "    model.add(layers.Dense(n_classes, activation=activations.softmax));\n",
        "\n",
        "    return model;"
      ],
      "metadata": {
        "id": "gNYnFH-VcMeF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VGGFakeBN(n_classes: int) -> Model:\n",
        "    model = keras.Sequential();\n",
        "    model.add(keras.Input((64, 64, 3)));\n",
        "\n",
        "    model.add(layers.Conv2D(32, (3, 3)));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(64, (3, 3)));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.Conv2D(64, (3, 3)));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "    model.add(layers.Conv2D(128, (3, 3)));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.Conv2D(128, (3, 3)));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.MaxPooling2D((2, 2)));\n",
        "\n",
        "    model.add(layers.Flatten());\n",
        "\n",
        "    model.add(layers.Dense(1024));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.Dropout(0.2));\n",
        "    model.add(layers.Dense(1024));\n",
        "    model.add(layers.BatchNormalization());\n",
        "    model.add(layers.ReLU());\n",
        "    model.add(layers.Dropout(0.2));\n",
        "    model.add(layers.Dense(n_classes, activation=activations.softmax));\n",
        "\n",
        "    return model;"
      ],
      "metadata": {
        "id": "qXpX3YB7gO6N"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "71FagJRXMPDg"
      },
      "outputs": [],
      "source": [
        "def getModel(model_fn: Callable[[int], Model], n_classes: int, optimizer: Optimizer) -> Model:\n",
        "    model: Model = model_fn(n_classes=n_classes);\n",
        "    model.compile(optimizer=optimizer,\n",
        "                loss=losses.SparseCategoricalCrossentropy(),\n",
        "                run_eagerly=False,\n",
        "                metrics=[\n",
        "                    metrics.SparseCategoricalAccuracy(),\n",
        "                ],\n",
        "                );\n",
        "    return model;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Skr1YbECJH"
      },
      "source": [
        "## train, evaluate and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Un3PfV956Mw"
      },
      "outputs": [],
      "source": [
        "def trainModel(model: Model, train_dataset, valid_dataset, batch_size: int, epochs: int) -> Any:\n",
        "    return model.fit(x=train_dataset,\n",
        "                     epochs=epochs,\n",
        "                     validation_data=valid_dataset,\n",
        "                     shuffle=False,\n",
        "                     );\n",
        "\n",
        "def testModel(model: Model, test_dataset, batch_size: int) -> Any:\n",
        "    return model.evaluate(x=test_dataset,\n",
        "                          );\n",
        "\n",
        "def predictModel(model: Model, x_test) -> np.ndarray:\n",
        "    return model.predict(x=x_test,\n",
        "                         batch_size=1,\n",
        "                         );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnoJiykI56Mx"
      },
      "source": [
        "## save and load weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZZzSWSXU56Mx"
      },
      "outputs": [],
      "source": [
        "def saveWeight(model: Model, filename: str) -> None:\n",
        "    filepath = gl_weight_directory + filename + \".weights.h5\";\n",
        "    model.save_weights(filepath=filepath, overwrite=True);\n",
        "\n",
        "def loadWeight(model: Model, filename: str) -> None:\n",
        "    filepath = gl_weight_directory + filename + \".weights.h5\";\n",
        "    model.load_weights(filepath=filepath, skip_mismatch=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PU0emSv56Mx"
      },
      "source": [
        "## test inference image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ynDxtAGv56Mx"
      },
      "outputs": [],
      "source": [
        "def getImages() -> np.ndarray:\n",
        "    import matplotlib.image as mpimg;\n",
        "    images = [None] * gl_num_images;\n",
        "    for i in range(gl_num_images):\n",
        "        filename = gl_testimages_directory + str(i + 1) + '.png';\n",
        "        img = cv2.imread(filename, cv2.IMREAD_COLOR); # rgb, 0~255, int\n",
        "        images[i] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB);\n",
        "    images = np.array(images);\n",
        "    return images;\n",
        "\n",
        "def getSignNames() -> np.ndarray:\n",
        "    import pandas;\n",
        "    df = pandas.read_csv(gl_signnames_file)['SignName'];\n",
        "    return df.to_numpy();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FYFL2TXF56Mz"
      },
      "outputs": [],
      "source": [
        "# Load the images and plot them here.\n",
        "def inference(x_real: np.ndarray, y_real: np.ndarray, sign_names: np.ndarray, model: keras.models.Sequential) -> None:\n",
        "    prediction = predictModel(model, x_real);\n",
        "    prediction_class = np.argmax(prediction, axis=1);\n",
        "    prediction_value = np.max(prediction, axis=1);\n",
        "    correct_count = 0;\n",
        "\n",
        "    for i in range(gl_num_images):\n",
        "        printfmt = \"predicted class = {} ({}), value={:.4f}\\nactual class = {} ({}).\";\n",
        "        if prediction_class[i] == y_real[i]:\n",
        "            correct_count += 1;\n",
        "        printstring = printfmt.format(prediction_class[i],\n",
        "                                      sign_names[prediction_class[i]],\n",
        "                                      prediction_value[i],\n",
        "                                      y_real[i],\n",
        "                                      sign_names[y_real[i]],\n",
        "                                      );\n",
        "        print(printstring);\n",
        "    print(\"inference accuracy: {}/{}\".format(correct_count, gl_num_images));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0acwzmy56Mz"
      },
      "source": [
        "# main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0ufB2puzXxv7"
      },
      "outputs": [],
      "source": [
        "def main() -> None:\n",
        "    \"\"\"\n",
        "    main function\n",
        "    \"\"\"\n",
        "    # start\n",
        "    print(\"begin\");\n",
        "\n",
        "    # hyperparameters\n",
        "    batch_size = 16;\n",
        "    epochs = 30;\n",
        "\n",
        "    # do final test / inference or not\n",
        "    do_final_test = True;\n",
        "    do_inference = True;\n",
        "\n",
        "    # load dataset\n",
        "    X_train_raw, Y_train_raw = loadDataset(\"train\");\n",
        "    X_valid_raw, Y_valid_raw = loadDataset(\"valid\");\n",
        "    X_test_raw, Y_test_raw = loadDataset(\"test\");\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_raw, Y_train_raw));\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid_raw, Y_valid_raw));\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test_raw, Y_test_raw));\n",
        "    X_inference = getImages();\n",
        "    Y_inference = np.array([17,12,14,11,38,4,35,33,25,13]);\n",
        "    num_classes = len(np.unique(Y_train_raw)); # = 43\n",
        "\n",
        "    # optimizer needs seperate instance\n",
        "    model_list = [\n",
        "        getModel(model_fn=Lenet, n_classes=num_classes, optimizer=keras.optimizers.Adam(learning_rate=0.001)),\n",
        "        getModel(model_fn=VGGFake, n_classes=num_classes, optimizer=keras.optimizers.Adam(learning_rate=0.001)),\n",
        "        getModel(model_fn=VGGFakePlus, n_classes=num_classes, optimizer=keras.optimizers.Adam(learning_rate=0.001)),\n",
        "        getModel(model_fn=VGGFakeBN, n_classes=num_classes, optimizer=keras.optimizers.Adam(learning_rate=0.001)),\n",
        "    ];\n",
        "    model_name_list = [\n",
        "        \"lenet_adam_v1\",\n",
        "        \"vgg_adam_v1\",\n",
        "        \"vggplus_adam_v1\",\n",
        "        \"vggbn_adam_v1\",\n",
        "    ];\n",
        "\n",
        "    for model, model_name in zip(model_list, model_name_list):\n",
        "        i_s = model.input_shape;\n",
        "        input_size = (i_s[1], i_s[2], i_s[3]);\n",
        "\n",
        "        # augment dataset, I can't extract this because a lot of function contains singleton variables\n",
        "        # singleton (global?) variables prevents reusability\n",
        "\n",
        "        train_ds = datasetTransform(train_dataset,\n",
        "                                    lambda x, y: augment(x, y, input_size),\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True);\n",
        "        valid_ds = datasetTransform(valid_dataset,\n",
        "                                    lambda x, y: preprocess(x, y, input_size),\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=False);\n",
        "        test_ds = datasetTransform(test_dataset,\n",
        "                                   lambda x, y: preprocess(x, y, input_size),\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=False);\n",
        "\n",
        "        X_inference_proc, _ = preprocess(X_inference, Y_inference, input_size=input_size);\n",
        "\n",
        "        # load weight(optional)\n",
        "        loadWeight(model, model_name);\n",
        "\n",
        "        # train model\n",
        "        #trainModel(model, train_ds, valid_ds, batch_size, epochs);\n",
        "\n",
        "        # save weight(optional)\n",
        "        #saveWeight(model, model_name);\n",
        "\n",
        "        # evaluate model\n",
        "        print(\"=====evaluate model result:======\");\n",
        "        testModel(model, valid_ds, batch_size);\n",
        "        print(\"=================================\");\n",
        "\n",
        "        # test model\n",
        "        if do_final_test:\n",
        "            print(\"=======test model result:=======\");\n",
        "            testModel(model, test_ds, batch_size);\n",
        "            print(\"================================\");\n",
        "\n",
        "        # inference\n",
        "        if do_inference:\n",
        "            sign_names = getSignNames();\n",
        "            inference(x_real=X_inference_proc,\n",
        "                    y_real=Y_inference,\n",
        "                    sign_names=sign_names,\n",
        "                    model=model,\n",
        "                    );\n",
        "\n",
        "    # finish\n",
        "    print(\"end\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B6Vg70LLx6I",
        "outputId": "4073262e-9c27-4f0c-abf1-fadeb373ed61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version matched, install skipped.\n",
            "begin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 21 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====evaluate model result:======\n",
            "276/276 [==============================] - 1s 3ms/step - loss: 0.4532 - sparse_categorical_accuracy: 0.8741\n",
            "=================================\n",
            "=======test model result:=======\n",
            "790/790 [==============================] - 4s 5ms/step - loss: 0.5118 - sparse_categorical_accuracy: 0.8804\n",
            "================================\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "predicted class = 17 (No entry), value=0.9984\n",
            "actual class = 17 (No entry).\n",
            "predicted class = 12 (Priority road), value=0.9984\n",
            "actual class = 12 (Priority road).\n",
            "predicted class = 14 (Stop), value=0.6933\n",
            "actual class = 14 (Stop).\n",
            "predicted class = 11 (Right-of-way at the next intersection), value=0.9947\n",
            "actual class = 11 (Right-of-way at the next intersection).\n",
            "predicted class = 38 (Keep right), value=0.9999\n",
            "actual class = 38 (Keep right).\n",
            "predicted class = 4 (Speed limit (70km/h)), value=0.8443\n",
            "actual class = 4 (Speed limit (70km/h)).\n",
            "predicted class = 35 (Ahead only), value=0.9893\n",
            "actual class = 35 (Ahead only).\n",
            "predicted class = 33 (Turn right ahead), value=0.9973\n",
            "actual class = 33 (Turn right ahead).\n",
            "predicted class = 30 (Beware of ice/snow), value=0.3859\n",
            "actual class = 25 (Road work).\n",
            "predicted class = 13 (Yield), value=0.9990\n",
            "actual class = 13 (Yield).\n",
            "inference accuracy: 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 33 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====evaluate model result:======\n",
            "276/276 [==============================] - 1s 4ms/step - loss: 0.2808 - sparse_categorical_accuracy: 0.9741\n",
            "=================================\n",
            "=======test model result:=======\n",
            "790/790 [==============================] - 3s 4ms/step - loss: 0.3712 - sparse_categorical_accuracy: 0.9572\n",
            "================================\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "predicted class = 17 (No entry), value=1.0000\n",
            "actual class = 17 (No entry).\n",
            "predicted class = 12 (Priority road), value=1.0000\n",
            "actual class = 12 (Priority road).\n",
            "predicted class = 14 (Stop), value=1.0000\n",
            "actual class = 14 (Stop).\n",
            "predicted class = 11 (Right-of-way at the next intersection), value=1.0000\n",
            "actual class = 11 (Right-of-way at the next intersection).\n",
            "predicted class = 38 (Keep right), value=1.0000\n",
            "actual class = 38 (Keep right).\n",
            "predicted class = 4 (Speed limit (70km/h)), value=1.0000\n",
            "actual class = 4 (Speed limit (70km/h)).\n",
            "predicted class = 35 (Ahead only), value=1.0000\n",
            "actual class = 35 (Ahead only).\n",
            "predicted class = 33 (Turn right ahead), value=1.0000\n",
            "actual class = 33 (Turn right ahead).\n",
            "predicted class = 25 (Road work), value=0.8279\n",
            "actual class = 25 (Road work).\n",
            "predicted class = 13 (Yield), value=1.0000\n",
            "actual class = 13 (Yield).\n",
            "inference accuracy: 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 33 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====evaluate model result:======\n",
            "276/276 [==============================] - 1s 4ms/step - loss: 0.0867 - sparse_categorical_accuracy: 0.9764\n",
            "=================================\n",
            "=======test model result:=======\n",
            "790/790 [==============================] - 5s 6ms/step - loss: 0.2510 - sparse_categorical_accuracy: 0.9595\n",
            "================================\n",
            "10/10 [==============================] - 0s 3ms/step\n",
            "predicted class = 17 (No entry), value=1.0000\n",
            "actual class = 17 (No entry).\n",
            "predicted class = 12 (Priority road), value=0.9336\n",
            "actual class = 12 (Priority road).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 61 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted class = 14 (Stop), value=1.0000\n",
            "actual class = 14 (Stop).\n",
            "predicted class = 11 (Right-of-way at the next intersection), value=1.0000\n",
            "actual class = 11 (Right-of-way at the next intersection).\n",
            "predicted class = 38 (Keep right), value=1.0000\n",
            "actual class = 38 (Keep right).\n",
            "predicted class = 4 (Speed limit (70km/h)), value=1.0000\n",
            "actual class = 4 (Speed limit (70km/h)).\n",
            "predicted class = 35 (Ahead only), value=1.0000\n",
            "actual class = 35 (Ahead only).\n",
            "predicted class = 33 (Turn right ahead), value=1.0000\n",
            "actual class = 33 (Turn right ahead).\n",
            "predicted class = 35 (Ahead only), value=0.5467\n",
            "actual class = 25 (Road work).\n",
            "predicted class = 13 (Yield), value=1.0000\n",
            "actual class = 13 (Yield).\n",
            "inference accuracy: 9/10\n",
            "=====evaluate model result:======\n",
            "276/276 [==============================] - 1s 4ms/step - loss: 0.0817 - sparse_categorical_accuracy: 0.9859\n",
            "=================================\n",
            "=======test model result:=======\n",
            "790/790 [==============================] - 4s 5ms/step - loss: 0.0964 - sparse_categorical_accuracy: 0.9815\n",
            "================================\n",
            "10/10 [==============================] - 0s 3ms/step\n",
            "predicted class = 17 (No entry), value=1.0000\n",
            "actual class = 17 (No entry).\n",
            "predicted class = 12 (Priority road), value=1.0000\n",
            "actual class = 12 (Priority road).\n",
            "predicted class = 14 (Stop), value=1.0000\n",
            "actual class = 14 (Stop).\n",
            "predicted class = 11 (Right-of-way at the next intersection), value=1.0000\n",
            "actual class = 11 (Right-of-way at the next intersection).\n",
            "predicted class = 38 (Keep right), value=1.0000\n",
            "actual class = 38 (Keep right).\n",
            "predicted class = 4 (Speed limit (70km/h)), value=0.8023\n",
            "actual class = 4 (Speed limit (70km/h)).\n",
            "predicted class = 35 (Ahead only), value=1.0000\n",
            "actual class = 35 (Ahead only).\n",
            "predicted class = 33 (Turn right ahead), value=1.0000\n",
            "actual class = 33 (Turn right ahead).\n",
            "predicted class = 11 (Right-of-way at the next intersection), value=0.7156\n",
            "actual class = 25 (Road work).\n",
            "predicted class = 13 (Yield), value=1.0000\n",
            "actual class = 13 (Yield).\n",
            "inference accuracy: 9/10\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    installEnvWrapper();\n",
        "    main();"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jUEHFcHoQFu9",
        "zmyWe6y2Bz1Y",
        "uhYRehM7CkTi",
        "r2ifhicnU2ZA",
        "V4Skr1YbECJH",
        "BnoJiykI56Mx",
        "-PU0emSv56Mx"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}